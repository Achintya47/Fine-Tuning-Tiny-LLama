{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ded58b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers\n",
    "# pip install datasets\n",
    "# pip install lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27b5ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test imports one by one\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"✅ transformers: {transformers.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ transformers error: {e}\")\n",
    "\n",
    "try:\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    print(\"✅ AutoModel imports successful\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ AutoModel error: {e}\")\n",
    "\n",
    "try:\n",
    "    from transformers import Trainer\n",
    "    print(\"✅ Trainer import successful\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Trainer error: {e}\")\n",
    "\n",
    "try:\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    print(\"✅ BitsAndBytesConfig import successful\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ BitsAndBytesConfig error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a6948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "def tokenize_example(example):\n",
    "    prompt = f\"\"\"### Instruction:\\n{example[\"instruction\"]}\\n\\n### Response:\\n{example[\"output\"]}\"\"\"\n",
    "    tokenized = tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=512)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996e9886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Load from Hugging Face Hub\n",
    "raw_dataset = load_dataset(\"Josephgflowers/Finance-Instruct-500k\", split=\"train\")\n",
    "\n",
    "# Inspect structure (one sample)\n",
    "print(raw_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390ba2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_instruction_format(example):\n",
    "    return {\n",
    "        \"instruction\": example[\"user\"],\n",
    "        \"input\": \"\",\n",
    "        \"output\": example[\"assistant\"]\n",
    "    }\n",
    "\n",
    "# Apply mapping\n",
    "instruction_dataset = raw_dataset.map(convert_to_instruction_format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6f1ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example):\n",
    "    prompt = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['output']}\"\n",
    "    tokenized = tokenizer(prompt, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = instruction_dataset.map(tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b907ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "# from peft import prepare_model_for_kbit_training, get_peft_model, LoraConfig\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",            # use efficient quant type\n",
    "#     bnb_4bit_use_double_quant=True,       # double quantization\n",
    "#     bnb_4bit_compute_dtype=\"float16\"      # for better numeric stability\n",
    "# )\n",
    "\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "\n",
    "# base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "# lora_config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=32,\n",
    "#     target_modules=[\"q_proj\", \"v_proj\"],\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\"\n",
    "# )\n",
    "\n",
    "# model = get_peft_model(base_model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4041c44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./finance_adapter\",\n",
    "#     per_device_train_batch_size=4,\n",
    "#     gradient_accumulation_steps=4,\n",
    "#     num_train_epochs=3,\n",
    "#     learning_rate=2e-4,\n",
    "#     logging_steps=10,\n",
    "#     logging_dir=\"./finance_adapter/logs\",   # <- logs directory\n",
    "#     save_strategy=\"epoch\",\n",
    "#     fp16=True,\n",
    "#     report_to=\"tensorboard\",                # <- enable TensorBoard\n",
    "# )\n",
    "\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     train_dataset=tokenized_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "#     args=training_args,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77290955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# LoRA config (no quantization)\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351040fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finance_adapter\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    logging_dir=\"./finance_adapter/logs\",\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,  # Turn off if you're on CPU\n",
    "    report_to=\"tensorboard\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209360e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(\"Trainable:\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f512b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained(\"finance_adapter\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaf9aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./finance_adapter/logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d735160",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, \"finance_adapter\")\n",
    "\n",
    "prompt = \"### Instruction:\\nWhat is working capital?\\n\\n### Response:\\n\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "output_ids = model.generate(input_ids, max_new_tokens=100, do_sample=True, temperature=0.7)\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
