<!-- Hugging Face + Project Header -->
<p align="center">
  <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" width="120"/>
  <h1 align="center">Finance Assistant LLM with LoRA Adapter Fine-Tuning</h1>
  <p align="center">
    Fine-tuned <b>TinyLlama-1.1B-Chat</b> on the <i>Finance-Instruct-500k</i> dataset using efficient adapter-based training.
  </p>
</p>

---

## ðŸ”§ Project Overview

This project demonstrates how to fine-tune a lightweight language model using [PEFT](https://github.com/huggingface/peft)'s **LoRA** (Low-Rank Adaptation) on financial instruction data.

- âœ… Efficient fine-tuning with LoRA adapters (only ~0.1% of weights trained)
- âœ… Uses [TinyLlama-1.1B-Chat](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)
- âœ… Trained on finance domain instructions: definitions, comparisons, concepts
- âœ… Hugging Face ecosystem: `transformers`, `peft`, `datasets`

---

## ðŸ§  Model Architecture

- **Base model**: `TinyLlama-1.1B-Chat-v1.0`
- **Training method**: LoRA Adapter Training (`r=16`, `alpha=32`, `dropout=0.05`)
- **Target modules**: `q_proj`, `v_proj`
- **Precision**: FP16 (no quantization)

---

## ðŸ“¦ Dataset

Dataset used:

- ðŸ“‚ [Finance-Instruct-500k](https://huggingface.co/datasets/Josephgflowers/Finance-Instruct-500k)
- Instruction-based financial Q&A pairs

> Example:
```json
{
  "system": "You are a financial assistant.",
  "user": "What is the difference between stocks and bonds?",
  "assistant": "Stocks represent ownership in a company, while bonds are a form of debt."
}
```

## ðŸ“š Citation

```bibtex
@dataset{josephgflowers2025financeinstruct,
  title={Finance-Instruct-500k},
  author={Joseph G. Flowers},
  year={2025},
  url={https://huggingface.co/datasets/Josephgflowers/Finance-Instruct-500k}
}
```
